using sklearn.metrics.adjusted_mutual_info_score

n = A.shape[1]
for ix = arange(n)
for jx = arange(ix+1, n):
    matMI[ix, jx] = calc_MI(A[:, ix], A[:, jx])

    import numpy as np

    from scipy.stats import chi2_contingency


    def calc_MI(x, y, bins):
        c_xy = np.histogram2d(x, y, bins)[0]
        g, p, dof, expected = chi2_contingency(c_xy, lambda_="log-likelihood")
        mi = 0.5 * g / c_xy.sum()
        return mi


    def shan_entropy(c):
        c_normalized = c / float(np.sum(c))
        c_normalized = c_normalized[np.nonzero(c_normalized)]
        H = -sum(c_normalized * np.log2(c_normalized))
        return H


    A = np.array([[2.0, 140.0, 128.23, -150.5, -5.4],
                  [2.4, 153.11, 130.34, -130.1, -9.5],
                  [1.2, 156.9, 120.11, -110.45, -1.12]])

    bins = 5  # ?
    n = A.shape[1]
    matMI = np.zeros((n, n))

    for ix in np.arange(n):
        for jx in np.arange(ix + 1, n):
            matMI[ix, jx] = calc_MI(A[:, ix], A[:, jx], bins)
